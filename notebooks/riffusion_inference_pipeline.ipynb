{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import io\n",
    "import typing as T\n",
    "\n",
    "from uuid import uuid4\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydub\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "\n",
    "from tools.riffusion.spectrogram_params import SpectrogramParams\n",
    "from tools.riffusion.spectrogram_image_converter import SpectrogramImageConverter\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.backends.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "DTYPE = torch.float32\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "EXPERIMENT_DATE = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "OUTPUT_FOLDER = f\"experiments/{EXPERIMENT_DATE}\"\n",
    "\n",
    "EXPERIMENT_ID = str(uuid4())\n",
    "\n",
    "FILE_PATH = \"test_reference_long.wav\"\n",
    "SAMPLING_RATE = 44100\n",
    "\n",
    "DENOISING_STRENGTH = 0.45\n",
    "GUIDANCE_SCALE = 7.0\n",
    "NUM_INFERENCE_STEPS = 50\n",
    "SEED = 42\n",
    "\n",
    "PROMPT = \"classic italian tenor operatic pop\"\n",
    "NEGATIVE_PROMPT = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment = pydub.AudioSegment.from_file(FILE_PATH)\n",
    "segment = segment.set_frame_rate(SAMPLING_RATE)\n",
    "print(f\"Duration: {segment.duration_seconds:.2f}s, Sample Rate: {segment.frame_rate}Hz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TIME = 0.0\n",
    "DURATION = segment.duration_seconds\n",
    "CLIP_DURATION = 5.0\n",
    "OVERLAP_DURATION = 2.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = min(DURATION, segment.duration_seconds - START_TIME)\n",
    "increment = CLIP_DURATION - OVERLAP_DURATION\n",
    "clip_start_times = START_TIME + np.arange(0, DURATION - CLIP_DURATION, increment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    {\n",
    "        \"Start Time [s]\": clip_start_times,\n",
    "        \"End Time [s]\": clip_start_times + CLIP_DURATION,\n",
    "        \"Duration [s]\": CLIP_DURATION,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_segments: T.List[pydub.AudioSegment] = []\n",
    "\n",
    "for i, clip_start_time in enumerate(clip_start_times):\n",
    "    clip_start_time_ms = int(clip_start_time * 1000)\n",
    "    clip_duration_ms = int(CLIP_DURATION * 1000)\n",
    "    clip_segment = segment[clip_start_time_ms : clip_start_time_ms + clip_duration_ms]\n",
    "\n",
    "    if i == len(clip_start_times) - 1:\n",
    "        silence_ms = clip_duration_ms - int(clip_segment.duration_seconds * 1000)\n",
    "        if silence_ms > 0:\n",
    "            clip_segment = clip_segment.append(\n",
    "                pydub.AudioSegment.silent(duration=silence_ms)\n",
    "            )\n",
    "\n",
    "    clip_segments.append(clip_segment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all(\n",
    "    [\n",
    "        type(clip_segment) == pydub.audio_segment.AudioSegment\n",
    "        for clip_segment in clip_segments\n",
    "    ]\n",
    "):\n",
    "    print(\"Segments created successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = SpectrogramParams()\n",
    "\n",
    "image_converter = SpectrogramImageConverter(params=params, device=device)\n",
    "\n",
    "result_images: T.List[Image.Image] = []\n",
    "result_segments: T.List[pydub.AudioSegment] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"riffusion/riffusion-model-v1\",\n",
    "    revision=\"main\",\n",
    "    torch_dtype=DTYPE,\n",
    "    safety_checker=lambda images, **kwargs: (images, [False] * len(images)),\n",
    ").to(device)\n",
    "\n",
    "generator_device = \"cpu\" if device.lower().startswith(\"mps\") else device\n",
    "generator = torch.Generator(device=generator_device).manual_seed(SEED)\n",
    "\n",
    "num_expected_steps = max(int(NUM_INFERENCE_STEPS * DENOISING_STRENGTH), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_images = list()\n",
    "result_segments = list()\n",
    "\n",
    "for clip_segment in clip_segments:\n",
    "    # TODO: implement intermediary saving of the source clip here if needed\n",
    "    clear_output(wait=False)\n",
    "    print(f\"Processing clip {len(result_images) + 1}/{len(clip_segments)}...\")\n",
    "\n",
    "    init_image = image_converter.spectrogram_image_from_audio(clip_segment)\n",
    "\n",
    "    closest_width = int(np.ceil(init_image.width / 32) * 32)\n",
    "    closest_height = int(np.ceil(init_image.height / 32) * 32)\n",
    "    init_image_resized = init_image.resize(\n",
    "        (closest_width, closest_height), Image.BICUBIC\n",
    "    )\n",
    "\n",
    "    result = pipeline(\n",
    "        prompt=PROMPT,\n",
    "        image=init_image,\n",
    "        strength=DENOISING_STRENGTH,\n",
    "        num_inference_steps=NUM_INFERENCE_STEPS,\n",
    "        guidance_scale=GUIDANCE_SCALE,\n",
    "        negative_prompt=NEGATIVE_PROMPT or None,\n",
    "        num_images_per_prompt=1,\n",
    "        generator=generator,\n",
    "        callback=None,\n",
    "        callback_steps=1,\n",
    "    )\n",
    "    result_image = result[0][0]\n",
    "    result_image = result_image.resize(init_image.size, Image.BICUBIC)\n",
    "\n",
    "    result_segment = image_converter.audio_from_spectrogram_image(result_image)\n",
    "\n",
    "    result_images.append(result_image)\n",
    "    result_segments.append(result_segment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine clips with a crossfade based on overlap\n",
    "crossfade_ms = int(OVERLAP_DURATION * 1000)\n",
    "combined_segment = result_segments[0]\n",
    "for segment in result_segments[1:]:\n",
    "    combined_segment = combined_segment.append(segment, crossfade=crossfade_ms)\n",
    "\n",
    "print(f\"#### Final Audio ({combined_segment.duration_seconds}s)\")\n",
    "combined_segment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_segment.export(f\"{OUTPUT_FOLDER}/{EXPERIMENT_ID}.wav\", format=\"wav\")\n",
    "\n",
    "with open(f\"{OUTPUT_FOLDER}/{EXPERIMENT_ID}.txt\", \"w\") as f:\n",
    "    f.write(f\"Experiment date: {EXPERIMENT_DATE}\\n\")\n",
    "    f.write(f\"Experiment ID: {EXPERIMENT_ID}\\n\")\n",
    "    f.write(f\"Prompt: {PROMPT}\\n\")\n",
    "    f.write(f\"Negative prompt: {NEGATIVE_PROMPT}\\n\")\n",
    "    f.write(f\"Denoising strength: {DENOISING_STRENGTH}\\n\")\n",
    "    f.write(f\"Guidance scale: {GUIDANCE_SCALE}\\n\")\n",
    "    f.write(f\"Number of inference steps: {NUM_INFERENCE_STEPS}\\n\")\n",
    "    f.write(f\"Seed: {SEED}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e47a413293bc6fe038b650abb2105665fccf8a01e5d9bbe3f5d2d04c50f645ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
